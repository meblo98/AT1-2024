{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGS1wSj6CB7yEgvg6GYEa3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meblo98/AT1-2024/blob/main/D%C3%A9s%C3%A9quilibr%C3%A9es_et_Analyse_par_Odds_Ratio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WjRUWsD5jwxT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "2e6b267d-7522-4e55-a205-2712ccf3fba1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'depression_dataset_SHAP_PEACE (1).zip'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e9af6477e6ed>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Extraction du fichier ZIP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mzip_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'depression_dataset_SHAP_PEACE (1).zip'\u001b[0m  \u001b[0;31m# À adapter selon le chemin sur votre machine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'depression_dataset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[1;32m   1293\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'depression_dataset_SHAP_PEACE (1).zip'"
          ]
        }
      ],
      "source": [
        "# Cellule 1: Importation des bibliothèques nécessaires\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.svm import SVC\n",
        "import time\n",
        "import zipfile\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Cellule 2: Chargement et exploration du dataset\n",
        "# Extraction du fichier ZIP\n",
        "zip_path = 'depression_dataset_SHAP_PEACE (1).zip'  # À adapter selon le chemin sur votre machine\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('depression_dataset')\n",
        "\n",
        "# Charger le dataset (suppose un fichier CSV dans le ZIP)\n",
        "# Remplacez 'dataset.csv' par le nom réel du fichier extrait\n",
        "data = pd.read_csv('depression_dataset/dataset.csv')  # À adapter\n",
        "\n",
        "# Exploration initiale\n",
        "print(\"Aperçu des données:\")\n",
        "print(data.head())\n",
        "print(\"\\nColonnes:\", data.columns)\n",
        "print(\"\\nDistribution de la classe cible:\")\n",
        "print(data['target'].value_counts(normalize=True))  # Remplacez 'target' par le nom de la colonne cible\n",
        "\n",
        "# Supposons que 'target' est la colonne de dépression (0: non-déprimé, 1: déprimé)\n",
        "X = data.drop('target', axis=1)\n",
        "y = data['target']\n",
        "\n",
        "# Division en ensembles d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Standardisation des données\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Cellule 3: Fonction pour évaluer les modèles\n",
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    print(f\"\\nÉvaluation du modèle: {model_name}\")\n",
        "    print(classification_report(y_true, y_pred))\n",
        "\n",
        "    # Métriques pour la classe minoritaire (classe 1)\n",
        "    precision = precision_score(y_true, y_pred, pos_label=1)\n",
        "    recall = recall_score(y_true, y_pred, pos_label=1)\n",
        "    f1 = f1_score(y_true, y_pred, pos_label=1)\n",
        "\n",
        "    return {'precision': precision, 'recall': recall, 'f1': f1}\n",
        "\n",
        "# Cellule 4: Modèle conçu pour les données déséquilibrées (XGBoost avec scale_pos_weight)\n",
        "# Calcul du scale_pos_weight\n",
        "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
        "\n",
        "start_time = time.time()\n",
        "xgb_model = XGBClassifier(scale_pos_weight=scale_pos_weight, random_state=42)\n",
        "xgb_model.fit(X_train_scaled, y_train)\n",
        "xgb_pred = xgb_model.predict(X_test_scaled)\n",
        "xgb_time = time.time() - start_time\n",
        "\n",
        "xgb_metrics = evaluate_model(y_test, xgb_pred, \"XGBoost avec scale_pos_weight\")\n",
        "print(f\"Temps de calcul: {xgb_time:.2f} secondes\")\n",
        "\n",
        "# Cellule 5: Modèle supplémentaire (SVM avec class_weight='balanced')\n",
        "start_time = time.time()\n",
        "svm_model = SVC(class_weight='balanced', random_state=42)\n",
        "svm_model.fit(X_train_scaled, y_train)\n",
        "svm_pred = svm_model.predict(X_test_scaled)\n",
        "svm_time = time.time() - start_time\n",
        "\n",
        "svm_metrics = evaluate_model(y_test, svm_pred, \"SVM avec class_weight='balanced'\")\n",
        "print(f\"Temps de calcul: {svm_time:.2f} secondes\")\n",
        "\n",
        "# Cellule 6: SMOTE + PCA + Random Forest\n",
        "start_time = time.time()\n",
        "\n",
        "# Application de SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "# Application de PCA\n",
        "pca = PCA(n_components=0.95)  # Conserver 95% de la variance\n",
        "X_train_smote_pca = pca.fit_transform(X_train_smote)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# Random Forest\n",
        "rf_smote = RandomForestClassifier(random_state=42)\n",
        "rf_smote.fit(X_train_smote_pca, y_train_smote)\n",
        "rf_smote_pred = rf_smote.predict(X_test_pca)\n",
        "rf_smote_time = time.time() - start_time\n",
        "\n",
        "rf_smote_metrics = evaluate_model(y_test, rf_smote_pred, \"SMOTE + PCA + Random Forest\")\n",
        "print(f\"Temps de calcul: {rf_smote_time:.2f} secondes\")\n",
        "\n",
        "# Cellule 7: Borderline-SMOTE + Random Forest\n",
        "start_time = time.time()\n",
        "\n",
        "# Application de Borderline-SMOTE\n",
        "borderline_smote = BorderlineSMOTE(random_state=42)\n",
        "X_train_borderline, y_train_borderline = borderline_smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "# Random Forest\n",
        "rf_borderline = RandomForestClassifier(random_state=42)\n",
        "rf_borderline.fit(X_train_borderline, y_train_borderline)\n",
        "rf_borderline_pred = rf_borderline.predict(X_test_scaled)\n",
        "rf_borderline_time = time.time() - start_time\n",
        "\n",
        "rf_borderline_metrics = evaluate_model(y_test, rf_borderline_pred, \"Borderline-SMOTE + Random Forest\")\n",
        "print(f\"Temps de calcul: {rf_borderline_time:.2f} secondes\")\n",
        "\n",
        "# Cellule 8: Comparaison des résultats\n",
        "results = pd.DataFrame({\n",
        "    'Modèle': ['XGBoost', 'SVM', 'SMOTE + PCA + RF', 'Borderline-SMOTE + RF'],\n",
        "    'Precision': [xgb_metrics['precision'], svm_metrics['precision'], rf_smote_metrics['precision'], rf_borderline_metrics['precision']],\n",
        "    'Recall': [xgb_metrics['recall'], svm_metrics['recall'], rf_smote_metrics['recall'], rf_borderline_metrics['recall']],\n",
        "    'F1-Score': [xgb_metrics['f1'], svm_metrics['f1'], rf_smote_metrics['f1'], rf_borderline_metrics['f1']],\n",
        "    'Temps (s)': [xgb_time, svm_time, rf_smote_time, rf_borderline_time]\n",
        "})\n",
        "\n",
        "print(\"\\nComparaison des performances sur la classe minoritaire:\")\n",
        "print(results)\n",
        "\n",
        "# Visualisation\n",
        "plt.figure(figsize=(10, 6))\n",
        "results.set_index('Modèle')[['Precision', 'Recall', 'F1-Score']].plot(kind='bar')\n",
        "plt.title('Comparaison des métriques pour la classe minoritaire')\n",
        "plt.ylabel('Score')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Cellule 9: Analyse des critères de comparaison\n",
        "print(\"\\nAnalyse des critères:\")\n",
        "print(\"1. Précision, Rappel, F1-Score: Voir tableau ci-dessus\")\n",
        "print(\"2. Robustesse: XGBoost et Random Forest sont robustes pour les données déséquilibrées\")\n",
        "print(\"3. Temps de calcul: XGBoost est rapide, SVM plus lent, SMOTE ajoute du temps\")\n",
        "print(\"4. Facilité de mise en œuvre: XGBoost et Random Forest sont simples à configurer\")\n",
        "print(\"5. Capacité à traiter le déséquilibre: XGBoost et SVM gèrent bien sans rééchantillonnage\")\n",
        "\n",
        "print(\"\\nRéflexions:\")\n",
        "print(\"- Le rééchantillonnage n'est pas toujours nécessaire si les modèles pondèrent les classes.\")\n",
        "print(\"- Compromis: XGBoost peut favoriser la classe minoritaire au détriment de la précision globale.\")\n",
        "print(\"- Les pondérations de classe suffisent pour des déséquilibres modérés.\")\n",
        "\n",
        "# Cellule 10: Analyse des Odds Ratios\n",
        "# Supposons une variable 'PHQ9_score' (score de dépression) pour l'analyse\n",
        "# À adapter selon l'image et les variables du dataset\n",
        "print(\"\\nAnalyse des Odds Ratios:\")\n",
        "\n",
        "# Hypothèse basée sur une visualisation supposée\n",
        "print(\"Hypothèse: Les scores PHQ-9 élevés sont fortement associés à la dépression.\")\n",
        "\n",
        "# Création d'une variable binaire pour 'PHQ9_score' (au-dessus/en-dessous de la médiane)\n",
        "data['High_PHQ9'] = (data['PHQ9_score'] > data['PHQ9_score'].median()).astype(int)  # À adapter\n",
        "\n",
        "# Calcul des odds ratios\n",
        "def calculate_odds_ratio(data, feature, target):\n",
        "    contingency_table = pd.crosstab(data[feature], data[target])\n",
        "    odds_ratio = (contingency_table.iloc[1,1] * contingency_table.iloc[0,0]) / \\\n",
        "                 (contingency_table.iloc[1,0] * contingency_table.iloc[0,1])\n",
        "    return odds_ratio\n",
        "\n",
        "or_phq9 = calculate_odds_ratio(data, 'High_PHQ9', 'target')\n",
        "print(f\"Odds Ratio pour High_PHQ9: {or_phq9:.2f}\")\n",
        "\n",
        "# Interprétation\n",
        "print(\"\\nInterprétation:\")\n",
        "if or_phq9 > 1:\n",
        "    print(\"L'OR suggère que des scores PHQ-9 élevés sont associés à la dépression.\")\n",
        "else:\n",
        "    print(\"L'OR suggère que des scores PHQ-9 élevés ne sont pas fortement associés à la dépression.\")\n",
        "print(\"Si la visualisation montrait une forte corrélation visuelle mais un OR faible, cela pourrait indiquer un effet de confusion.\")\n",
        "print(\"Vérifiez les interactions avec d'autres variables (ex. : âge, genre).\")\n",
        "\n",
        "# Cellule 11: Conclusion\n",
        "print(\"\\nConclusion:\")\n",
        "print(\"XGBoost avec scale_pos_weight est efficace pour les données déséquilibrées sans rééchantillonnage.\")\n",
        "print(\"Les odds ratios peuvent révéler des contradictions dues à des effets de confusion.\")\n",
        "print(\"Pour des données très déséquilibrées, combiner pondération et rééchantillonnage peut être optimal.\")"
      ]
    }
  ]
}